diff -pruN spark-1.2.1-bin-hadoop2.4-orig/sbin/slaves.sh spark-1.2.1-bin-hadoop2.4-alternate-ssh/sbin/slaves.sh
--- spark-1.2.1-bin-hadoop2.4-orig/sbin/slaves.sh	2015-02-02 19:45:55.000000000 -0800
+++ spark-1.2.1-bin-hadoop2.4-alternate-ssh/sbin/slaves.sh	2015-09-24 11:00:18.557697000 -0700
@@ -25,6 +25,8 @@
 #     Default is ${SPARK_CONF_DIR}/slaves.
 #   SPARK_CONF_DIR  Alternate conf dir. Default is ${SPARK_HOME}/conf.
 #   SPARK_SLAVE_SLEEP Seconds to sleep between spawning remote commands.
+#   SPARK_SSH_CMD Specify an alternate remote shell command.
+#     Defaults to ssh if not specified.
 #   SPARK_SSH_OPTS Options passed to ssh when running remote commands.
 ##
 
@@ -79,19 +81,19 @@ if [ "$HOSTLIST" = "" ]; then
   fi
 fi
 
-
+RSH_CMD=${SPARK_SSH_CMD:-ssh}
 
 # By default disable strict host key checking
-if [ "$SPARK_SSH_OPTS" = "" ]; then
+if [ "$RSH_CMD" == "ssh" ] && [ "$SPARK_SSH_OPTS" = "" ]; then
   SPARK_SSH_OPTS="-o StrictHostKeyChecking=no"
 fi
 
 for slave in `echo "$HOSTLIST"|sed  "s/#.*$//;/^$/d"`; do
   if [ -n "${SPARK_SSH_FOREGROUND}" ]; then
-    ssh $SPARK_SSH_OPTS "$slave" $"${@// /\\ }" \
+    $RSH_CMD $SPARK_SSH_OPTS "$slave" $"${@// /\\ }" \
       2>&1 | sed "s/^/$slave: /"
   else
-    ssh $SPARK_SSH_OPTS "$slave" $"${@// /\\ }" \
+    $RSH_CMD $SPARK_SSH_OPTS "$slave" $"${@// /\\ }" \
       2>&1 | sed "s/^/$slave: /" &
   fi
   if [ "$SPARK_SLAVE_SLEEP" != "" ]; then
diff -pruN spark-1.2.1-bin-hadoop2.4-orig/sbin/spark-daemon.sh spark-1.2.1-bin-hadoop2.4-alternate-ssh/sbin/spark-daemon.sh
--- spark-1.2.1-bin-hadoop2.4-orig/sbin/spark-daemon.sh	2015-02-02 19:45:55.000000000 -0800
+++ spark-1.2.1-bin-hadoop2.4-alternate-ssh/sbin/spark-daemon.sh	2015-09-24 11:00:18.559697000 -0700
@@ -137,7 +137,8 @@ case $option in
 
     if [ "$SPARK_MASTER" != "" ]; then
       echo rsync from "$SPARK_MASTER"
-      rsync -a -e ssh --delete --exclude=.svn --exclude='logs/*' --exclude='contrib/hod/logs/*' $SPARK_MASTER/ "$SPARK_HOME"
+      RSH_CMD=${SPARK_SSH_CMD:-ssh}
+      rsync -a -e $RSH_CMD --delete --exclude=.svn --exclude='logs/*' --exclude='contrib/hod/logs/*' $SPARK_MASTER/ "$SPARK_HOME"
     fi
 
     spark_rotate_log "$log"
diff -pruN spark-1.2.1-bin-hadoop2.4-orig/sbin/spark-daemons.sh spark-1.2.1-bin-hadoop2.4-alternate-ssh/sbin/spark-daemons.sh
--- spark-1.2.1-bin-hadoop2.4-orig/sbin/spark-daemons.sh	2015-02-02 19:45:55.000000000 -0800
+++ spark-1.2.1-bin-hadoop2.4-alternate-ssh/sbin/spark-daemons.sh	2015-09-24 11:00:07.995270000 -0700
@@ -30,6 +30,24 @@ fi
 sbin=`dirname "$0"`
 sbin=`cd "$sbin"; pwd`
 
+# Check if --config is passed as an argument. It is an optional parameter.
+# Exit if the argument is not a directory.
+
+if [ "$1" == "--config" ]
+then
+  shift
+  conf_dir=$1
+  if [ ! -d "$conf_dir" ]
+  then
+    echo "ERROR : $conf_dir is not a directory"
+    echo $usage
+    exit 1
+  else
+    export SPARK_CONF_DIR=$conf_dir
+  fi
+  shift
+fi
+
 . "$sbin/spark-config.sh"
 
-exec "$sbin/slaves.sh" cd "$SPARK_HOME" \; "$sbin/spark-daemon.sh" "$@"
+exec "$sbin/slaves.sh" --config $SPARK_CONF_DIR cd "$SPARK_HOME" \; "$sbin/spark-daemon.sh" --config $SPARK_CONF_DIR "$@"
diff -pruN spark-1.2.1-bin-hadoop2.4-orig/sbin/start-slave.sh spark-1.2.1-bin-hadoop2.4-alternate-ssh/sbin/start-slave.sh
--- spark-1.2.1-bin-hadoop2.4-orig/sbin/start-slave.sh	2015-02-02 19:45:55.000000000 -0800
+++ spark-1.2.1-bin-hadoop2.4-alternate-ssh/sbin/start-slave.sh	2015-09-24 11:00:07.997269000 -0700
@@ -23,4 +23,27 @@
 sbin="`dirname "$0"`"
 sbin="`cd "$sbin"; pwd`"
 
-"$sbin"/spark-daemon.sh start org.apache.spark.deploy.worker.Worker "$@"
+# Check if --config is passed as an argument. It is an optional parameter.
+# Exit if the argument is not a directory.
+
+if [ "$1" == "--config" ]
+then
+  shift
+  conf_dir=$1
+  if [ ! -d "$conf_dir" ]
+  then
+    echo "ERROR : $conf_dir is not a directory"
+    echo $usage
+    exit 1
+  else
+    export SPARK_CONF_DIR=$conf_dir
+  fi
+  shift
+fi
+
+if [ "${SPARK_CONF_DIR}X" != "X" ]
+then
+    "$sbin"/spark-daemon.sh --config $SPARK_CONF_DIR start org.apache.spark.deploy.worker.Worker "$@"
+else
+    "$sbin"/spark-daemon.sh start org.apache.spark.deploy.worker.Worker "$@"
+fi
diff -pruN spark-1.2.1-bin-hadoop2.4-orig/sbin/start-slaves.sh spark-1.2.1-bin-hadoop2.4-alternate-ssh/sbin/start-slaves.sh
--- spark-1.2.1-bin-hadoop2.4-orig/sbin/start-slaves.sh	2015-02-02 19:45:55.000000000 -0800
+++ spark-1.2.1-bin-hadoop2.4-alternate-ssh/sbin/start-slaves.sh	2015-09-24 11:00:08.000266000 -0700
@@ -58,12 +58,12 @@ fi
 
 # Launch the slaves
 if [ "$SPARK_WORKER_INSTANCES" = "" ]; then
-  exec "$sbin/slaves.sh" cd "$SPARK_HOME" \; "$sbin/start-slave.sh" 1 "spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT"
+  exec "$sbin/slaves.sh" --config $SPARK_CONF_DIR cd "$SPARK_HOME" \; "$sbin/start-slave.sh" --config $SPARK_CONF_DIR 1 "spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT"
 else
   if [ "$SPARK_WORKER_WEBUI_PORT" = "" ]; then
     SPARK_WORKER_WEBUI_PORT=8081
   fi
   for ((i=0; i<$SPARK_WORKER_INSTANCES; i++)); do
-    "$sbin/slaves.sh" cd "$SPARK_HOME" \; "$sbin/start-slave.sh" $(( $i + 1 ))  "spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT" --webui-port $(( $SPARK_WORKER_WEBUI_PORT + $i ))
+    "$sbin/slaves.sh" --config $SPARK_CONF_DIR cd "$SPARK_HOME" \; "$sbin/start-slave.sh" --config $SPARK_CONF_DIR $(( $i + 1 ))  "spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT" --webui-port $(( $SPARK_WORKER_WEBUI_PORT + $i ))
   done
 fi
diff -pruN spark-1.2.1-bin-hadoop2.4-orig/sbin/stop-slaves.sh spark-1.2.1-bin-hadoop2.4-alternate-ssh/sbin/stop-slaves.sh
--- spark-1.2.1-bin-hadoop2.4-orig/sbin/stop-slaves.sh	2015-02-02 19:45:55.000000000 -0800
+++ spark-1.2.1-bin-hadoop2.4-alternate-ssh/sbin/stop-slaves.sh	2015-09-24 11:00:08.003262000 -0700
@@ -30,9 +30,9 @@ if [ -e "$sbin"/../tachyon/bin/tachyon ]
 fi
 
 if [ "$SPARK_WORKER_INSTANCES" = "" ]; then
-  "$sbin"/spark-daemons.sh stop org.apache.spark.deploy.worker.Worker 1
+  "$sbin"/spark-daemons.sh --config $SPARK_CONF_DIR stop org.apache.spark.deploy.worker.Worker 1
 else
   for ((i=0; i<$SPARK_WORKER_INSTANCES; i++)); do
-    "$sbin"/spark-daemons.sh stop org.apache.spark.deploy.worker.Worker $(( $i + 1 ))
+    "$sbin"/spark-daemons.sh --config $SPARK_CONF_DIR stop org.apache.spark.deploy.worker.Worker $(( $i + 1 ))
   done
 fi
