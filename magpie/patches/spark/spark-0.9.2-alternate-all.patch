diff -pruN spark-0.9.2-bin-hadoop2-orig/bin/compute-classpath.sh spark-0.9.2-bin-hadoop2/bin/compute-classpath.sh
--- spark-0.9.2-bin-hadoop2-orig/bin/compute-classpath.sh	2014-07-17 01:25:25.000000000 -0700
+++ spark-0.9.2-bin-hadoop2/bin/compute-classpath.sh	2015-09-25 11:02:19.625980000 -0700
@@ -25,14 +25,15 @@ SCALA_VERSION=2.10
 # Figure out where Spark is installed
 FWDIR="$(cd `dirname $0`/..; pwd)"
 
-# Load environment variables from conf/spark-env.sh, if it exists
-if [ -e "$FWDIR/conf/spark-env.sh" ] ; then
+# Load environment variables from spark-env.sh, if it exists
+if [ "$SPARK_CONF_DIRX" != "X" ] && [ -e "$SPARK_CONF_DIR/spark-env.sh" ] ; then
+  . $SPARK_CONF_DIR/spark-env.sh
+  CLASSPATH="$SPARK_CLASSPATH:$SPARK_CONF_DIR"
+elif [ -e "$FWDIR/conf/spark-env.sh" ] ; then
   . $FWDIR/conf/spark-env.sh
+  CLASSPATH="$SPARK_CLASSPATH:$FWDIR/conf"
 fi
 
-# Build up classpath
-CLASSPATH="$SPARK_CLASSPATH:$FWDIR/conf"
-
 # First check if we have a dependencies jar. If so, include binary classes with the deps jar
 if [ -f "$FWDIR"/assembly/target/scala-$SCALA_VERSION/spark-assembly*hadoop*-deps.jar ]; then
   CLASSPATH="$CLASSPATH:$FWDIR/core/target/scala-$SCALA_VERSION/classes"
diff -pruN spark-0.9.2-bin-hadoop2-orig/bin/run-example spark-0.9.2-bin-hadoop2/bin/run-example
--- spark-0.9.2-bin-hadoop2-orig/bin/run-example	2014-07-17 01:25:25.000000000 -0700
+++ spark-0.9.2-bin-hadoop2/bin/run-example	2015-09-25 11:02:19.627977000 -0700
@@ -30,8 +30,10 @@ FWDIR="$(cd `dirname $0`/..; pwd)"
 # Export this as SPARK_HOME
 export SPARK_HOME="$FWDIR"
 
-# Load environment variables from conf/spark-env.sh, if it exists
-if [ -e "$FWDIR/conf/spark-env.sh" ] ; then
+# Load environment variables from spark-env.sh, if it exists
+if [ "${SPARK_CONF_DIR}X" != "X" ] && [ -e "${SPARK_CONF_DIR}/spark-env.sh" ] ; then
+  . $SPARK_CONF_DIR/spark-env.sh
+elif [ -e "$FWDIR/conf/spark-env.sh" ] ; then
   . $FWDIR/conf/spark-env.sh
 fi
 
diff -pruN spark-0.9.2-bin-hadoop2-orig/bin/spark-class spark-0.9.2-bin-hadoop2/bin/spark-class
--- spark-0.9.2-bin-hadoop2-orig/bin/spark-class	2014-07-17 01:25:25.000000000 -0700
+++ spark-0.9.2-bin-hadoop2/bin/spark-class	2015-09-25 11:02:19.630971000 -0700
@@ -30,8 +30,10 @@ FWDIR="$(cd `dirname $0`/..; pwd)"
 # Export this as SPARK_HOME
 export SPARK_HOME="$FWDIR"
 
-# Load environment variables from conf/spark-env.sh, if it exists
-if [ -e "$FWDIR/conf/spark-env.sh" ] ; then
+# Load environment variables from spark-env.sh, if it exists
+if [ "$SPARK_CONF_DIRX" != "X" ] && [ -e "$SPARK_CONF_DIR/spark-env.sh" ] ; then
+  . $SPARK_CONF_DIR/spark-env.sh
+elif [ -e "$FWDIR/conf/spark-env.sh" ] ; then
   . $FWDIR/conf/spark-env.sh
 fi
 
diff -pruN spark-0.9.2-bin-hadoop2-orig/bin/spark-shell spark-0.9.2-bin-hadoop2/bin/spark-shell
--- spark-0.9.2-bin-hadoop2-orig/bin/spark-shell	2014-07-17 01:25:25.000000000 -0700
+++ spark-0.9.2-bin-hadoop2/bin/spark-shell	2015-09-25 11:02:19.632969000 -0700
@@ -47,8 +47,10 @@ done
 # Set MASTER from spark-env if possible
 DEFAULT_SPARK_MASTER_PORT=7077
 if [ -z "$MASTER" ]; then
-  if [ -e "$FWDIR/conf/spark-env.sh" ]; then
-    . "$FWDIR/conf/spark-env.sh"
+  if [ "${SPARK_CONF_DIR}X" != "X" ] && [ -e "${SPARK_CONF_DIR}/spark-env.sh" ] ; then
+    . $SPARK_CONF_DIR/spark-env.sh
+  elif [ -e "$FWDIR/conf/spark-env.sh" ] ; then
+    . $FWDIR/conf/spark-env.sh
   fi
   if [ "x" != "x$SPARK_MASTER_IP" ]; then
     if [ "y" != "y$SPARK_MASTER_PORT" ]; then
diff -pruN spark-0.9.2-bin-hadoop2-orig/sbin/slaves.sh spark-0.9.2-bin-hadoop2/sbin/slaves.sh
--- spark-0.9.2-bin-hadoop2-orig/sbin/slaves.sh	2014-07-17 01:25:25.000000000 -0700
+++ spark-0.9.2-bin-hadoop2/sbin/slaves.sh	2015-09-25 11:02:34.005582000 -0700
@@ -25,6 +25,8 @@
 #     Default is ${SPARK_CONF_DIR}/slaves.
 #   SPARK_CONF_DIR  Alternate conf dir. Default is ${SPARK_HOME}/conf.
 #   SPARK_SLAVE_SLEEP Seconds to sleep between spawning remote commands.
+#   SPARK_SSH_CMD Specify an alternate remote shell command.
+#     Defaults to ssh if not specified.
 #   SPARK_SSH_OPTS Options passed to ssh when running remote commands.
 ##
 
@@ -75,13 +77,15 @@ if [ "$HOSTLIST" = "" ]; then
   fi
 fi
 
+RSH_CMD=${SPARK_SSH_CMD:-ssh}
+
 # By default disable strict host key checking
-if [ "$SPARK_SSH_OPTS" = "" ]; then
+if [ "$RSH_CMD" == "ssh" ] && [ "$SPARK_SSH_OPTS" = "" ]; then
   SPARK_SSH_OPTS="-o StrictHostKeyChecking=no"
 fi
 
 for slave in `cat "$HOSTLIST"|sed  "s/#.*$//;/^$/d"`; do
- ssh $SPARK_SSH_OPTS $slave $"${@// /\\ }" \
+ $RSH_CMD $SPARK_SSH_OPTS $slave $"${@// /\\ }" \
    2>&1 | sed "s/^/$slave: /" &
  if [ "$SPARK_SLAVE_SLEEP" != "" ]; then
    sleep $SPARK_SLAVE_SLEEP
diff -pruN spark-0.9.2-bin-hadoop2-orig/sbin/spark-config.sh spark-0.9.2-bin-hadoop2/sbin/spark-config.sh
--- spark-0.9.2-bin-hadoop2-orig/sbin/spark-config.sh	2014-07-17 01:25:25.000000000 -0700
+++ spark-0.9.2-bin-hadoop2/sbin/spark-config.sh	2015-09-25 11:02:29.415181000 -0700
@@ -33,4 +33,4 @@ this="$config_bin/$script"
 
 export SPARK_PREFIX=`dirname "$this"`/..
 export SPARK_HOME=${SPARK_PREFIX}
-export SPARK_CONF_DIR="$SPARK_HOME/conf"
+export SPARK_CONF_DIR="${SPARK_CONF_DIR:-$SPARK_HOME/conf}"
diff -pruN spark-0.9.2-bin-hadoop2-orig/sbin/spark-daemon.sh spark-0.9.2-bin-hadoop2/sbin/spark-daemon.sh
--- spark-0.9.2-bin-hadoop2-orig/sbin/spark-daemon.sh	2014-07-17 01:25:25.000000000 -0700
+++ spark-0.9.2-bin-hadoop2/sbin/spark-daemon.sh	2015-09-25 11:02:34.007580000 -0700
@@ -141,7 +141,8 @@ case $startStop in
 
     if [ "$SPARK_MASTER" != "" ]; then
       echo rsync from $SPARK_MASTER
-      rsync -a -e ssh --delete --exclude=.svn --exclude='logs/*' --exclude='contrib/hod/logs/*' $SPARK_MASTER/ "$SPARK_HOME"
+      RSH_CMD=${SPARK_SSH_CMD:-ssh}
+      rsync -a -e $RSH_CMD --delete --exclude=.svn --exclude='logs/*' --exclude='contrib/hod/logs/*' $SPARK_MASTER/ "$SPARK_HOME"
     fi
 
     spark_rotate_log "$log"
diff -pruN spark-0.9.2-bin-hadoop2-orig/sbin/spark-daemons.sh spark-0.9.2-bin-hadoop2/sbin/spark-daemons.sh
--- spark-0.9.2-bin-hadoop2-orig/sbin/spark-daemons.sh	2014-07-17 01:25:25.000000000 -0700
+++ spark-0.9.2-bin-hadoop2/sbin/spark-daemons.sh	2015-09-25 11:02:29.417179000 -0700
@@ -30,6 +30,24 @@ fi
 sbin=`dirname "$0"`
 sbin=`cd "$sbin"; pwd`
 
+# Check if --config is passed as an argument. It is an optional parameter.
+# Exit if the argument is not a directory.
+
+if [ "$1" == "--config" ]
+then
+  shift
+  conf_dir=$1
+  if [ ! -d "$conf_dir" ]
+  then
+    echo "ERROR : $conf_dir is not a directory"
+    echo $usage
+    exit 1
+  else
+    export SPARK_CONF_DIR=$conf_dir
+  fi
+  shift
+fi
+
 . "$sbin/spark-config.sh"
 
-exec "$sbin/slaves.sh" cd "$SPARK_HOME" \; "$sbin/spark-daemon.sh" "$@"
+exec "$sbin/slaves.sh" --config $SPARK_CONF_DIR cd "$SPARK_HOME" \; "$sbin/spark-daemon.sh" --config $SPARK_CONF_DIR "$@"
diff -pruN spark-0.9.2-bin-hadoop2-orig/sbin/start-slave.sh spark-0.9.2-bin-hadoop2/sbin/start-slave.sh
--- spark-0.9.2-bin-hadoop2-orig/sbin/start-slave.sh	2014-07-17 01:25:25.000000000 -0700
+++ spark-0.9.2-bin-hadoop2/sbin/start-slave.sh	2015-09-25 11:02:29.420172000 -0700
@@ -32,4 +32,27 @@ if [ "$SPARK_PUBLIC_DNS" = "" ]; then
     fi
 fi
 
-"$sbin"/spark-daemon.sh start org.apache.spark.deploy.worker.Worker "$@"
+# Check if --config is passed as an argument. It is an optional parameter.
+# Exit if the argument is not a directory.
+
+if [ "$1" == "--config" ]
+then
+  shift
+  conf_dir=$1
+  if [ ! -d "$conf_dir" ]
+  then
+    echo "ERROR : $conf_dir is not a directory"
+    echo $usage
+    exit 1
+  else
+    export SPARK_CONF_DIR=$conf_dir
+  fi
+  shift
+fi
+
+if [ "${SPARK_CONF_DIR}X" != "X" ]
+then
+    "$sbin"/spark-daemon.sh --config $SPARK_CONF_DIR start org.apache.spark.deploy.worker.Worker "$@"
+else
+    "$sbin"/spark-daemon.sh start org.apache.spark.deploy.worker.Worker "$@"
+fi
diff -pruN spark-0.9.2-bin-hadoop2-orig/sbin/start-slaves.sh spark-0.9.2-bin-hadoop2/sbin/start-slaves.sh
--- spark-0.9.2-bin-hadoop2-orig/sbin/start-slaves.sh	2014-07-17 01:25:25.000000000 -0700
+++ spark-0.9.2-bin-hadoop2/sbin/start-slaves.sh	2015-09-25 11:02:29.422170000 -0700
@@ -60,12 +60,12 @@ fi
 
 # Launch the slaves
 if [ "$SPARK_WORKER_INSTANCES" = "" ]; then
-  exec "$sbin/slaves.sh" cd "$SPARK_HOME" \; "$sbin/start-slave.sh" 1 spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT
+  exec "$sbin/slaves.sh" --config $SPARK_CONF_DIR cd "$SPARK_HOME" \; "$sbin/start-slave.sh" --config $SPARK_CONF_DIR 1 spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT
 else
   if [ "$SPARK_WORKER_WEBUI_PORT" = "" ]; then
     SPARK_WORKER_WEBUI_PORT=8081
   fi
   for ((i=0; i<$SPARK_WORKER_INSTANCES; i++)); do
-    "$sbin/slaves.sh" cd "$SPARK_HOME" \; "$sbin/start-slave.sh" $(( $i + 1 ))  spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT --webui-port $(( $SPARK_WORKER_WEBUI_PORT + $i ))
+    "$sbin/slaves.sh" --config $SPARK_CONF_DIR cd "$SPARK_HOME" \; "$sbin/start-slave.sh" --config $SPARK_CONF_DIR $(( $i + 1 ))  spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT --webui-port $(( $SPARK_WORKER_WEBUI_PORT + $i )) 
   done
 fi
diff -pruN spark-0.9.2-bin-hadoop2-orig/sbin/stop-slaves.sh spark-0.9.2-bin-hadoop2/sbin/stop-slaves.sh
--- spark-0.9.2-bin-hadoop2-orig/sbin/stop-slaves.sh	2014-07-17 01:25:25.000000000 -0700
+++ spark-0.9.2-bin-hadoop2/sbin/stop-slaves.sh	2015-09-25 11:02:29.424168000 -0700
@@ -32,9 +32,9 @@ if [ -e "$sbin"/../tachyon/bin/tachyon ]
 fi
 
 if [ "$SPARK_WORKER_INSTANCES" = "" ]; then
-  "$sbin"/spark-daemons.sh stop org.apache.spark.deploy.worker.Worker 1
+  "$sbin"/spark-daemons.sh --config $SPARK_CONF_DIR stop org.apache.spark.deploy.worker.Worker 1
 else
   for ((i=0; i<$SPARK_WORKER_INSTANCES; i++)); do
-    "$sbin"/spark-daemons.sh stop org.apache.spark.deploy.worker.Worker $(( $i + 1 ))
+    "$sbin"/spark-daemons.sh --config $SPARK_CONF_DIR stop org.apache.spark.deploy.worker.Worker $(( $i + 1 ))
   done
 fi
